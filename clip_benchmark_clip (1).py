# -*- coding: utf-8 -*-
"""CLIP_benchmark_clip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ozmW6zgobIxPoXWhzZeEEMa8KAFmmDFm
"""

!wget http://images.cocodataset.org/zips/val2017.zip
!unzip -q val2017.zip

import os
import torch
import pandas as pd
from PIL import Image
from tqdm.auto import tqdm
from transformers import AutoProcessor, AutoModelForZeroShotImageClassification

CSV_PATH = "/content/COCO_val_mcq_llama3.1_rephrased.csv"
IMAGE_COL = "image_path"
LABEL_COL = "correct_answer"
CAPTION_COL_TEMPLATE = "caption_{}"

NUM_CHOICES = 4
IMAGE_ROOT = "."

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

processor = AutoProcessor.from_pretrained("openai/clip-vit-base-patch32")
model = AutoModelForZeroShotImageClassification.from_pretrained("openai/clip-vit-base-patch32").to(DEVICE)

model.eval()

df = pd.read_csv(CSV_PATH)


correct = 0
total = 0

prefix = 'data/coco/images/'
prefix_len = len(prefix)

all_preds = []
all_labels = []

for _, row in tqdm(df.iterrows(), total=len(df)):

    img_col_adapted = row[IMAGE_COL][prefix_len:]
    img_path = os.path.join(IMAGE_ROOT, img_col_adapted)
    image = Image.open(img_path).convert("RGB")

    captions = [row[CAPTION_COL_TEMPLATE.format(i)] for i in range(NUM_CHOICES)]

    inputs = processor(
        text=captions,
        images=image,
        return_tensors="pt",
        padding=True,
    ).to(DEVICE)

    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits_per_image
        pred_idx = logits.argmax(dim=-1).item()

    label = int(row[LABEL_COL])
    all_preds.append(pred_idx)
    all_labels.append(label)

    if pred_idx == label:
        correct += 1
    total += 1

accuracy = correct / total if total > 0 else 0.0
print(f"Accuracy: {accuracy:.4f}  ({correct}/{total})")

import os
import math
import pandas as pd
import torch
from PIL import Image
from tqdm.auto import tqdm
from transformers import AutoProcessor, AutoModelForZeroShotImageClassification

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
MODEL_ID = "openai/clip-vit-base-patch32"

processor = AutoProcessor.from_pretrained(MODEL_ID)
model = AutoModelForZeroShotImageClassification.from_pretrained(MODEL_ID).to(DEVICE).eval()


def resolve_image_path(p, image_roots=(".",), strip_prefixes=()):
    p_raw = str(p).strip()
    if not p_raw:
        return None

    if isinstance(image_roots, (str, os.PathLike)):
        roots = [str(image_roots)]
    else:
        roots = [str(r) for r in image_roots if str(r)]

    def _exists(path):
        return path is not None and os.path.exists(path)

    def _try(path):
        path = os.path.normpath(path)
        if _exists(path):
            return path
        for root in roots:
            cand = os.path.normpath(os.path.join(root, path))
            if _exists(cand):
                return cand
        return None

    found = _try(p_raw)
    if found:
        return found

    variants = set()

    for pref in strip_prefixes:
        if p_raw.startswith(pref):
            v = p_raw[len(pref):].lstrip("/\\")
            variants.add(v)

    for likely in ("/content/", "content/", "./content/"):
        if p_raw.startswith(likely):
            variants.add(p_raw[len(likely):].lstrip("/\\"))

    base = os.path.basename(p_raw)
    if base:
        variants.add(os.path.join("val2017", base))

    for v in variants:
        found = _try(v)
        if found:
            return found
    return None


def _coerce_label(label_raw, num_choices):
    if isinstance(label_raw, str):
        s = label_raw.strip().upper()
        if len(s) == 1 and s in "ABCDEFGHIJKLMNOPQRSTUVWXYZ":
            idx = ord(s) - ord("A")
            if 0 <= idx < num_choices:
                return idx

    lab = int(label_raw)
    if num_choices == 4 and lab in (1, 2, 3, 4):
        return lab - 1
    return lab


@torch.inference_mode()
def eval_mcq(
    csv_path,
    image_roots=(".",),
    image_col="image_path",
    label_col="correct_answer",
    caption_col_template="caption_{}",
    num_choices=4,
    strip_prefixes=(),
    max_examples=None,
    skip_missing=False,
):
    df = pd.read_csv(csv_path)
    if max_examples is not None:
        df = df.iloc[:max_examples].copy()

    needed = [image_col, label_col] + [caption_col_template.format(i) for i in range(num_choices)]
    missing = [c for c in needed if c not in df.columns]
    if missing:
        raise ValueError(
            f"Missing columns in {csv_path}: {missing}\n"
            f"Available columns: {list(df.columns)}"
        )

    correct = 0
    preds, labels = [], []
    used = 0

    for i, row in tqdm(df.iterrows(), total=len(df)):
        img_path = resolve_image_path(row[image_col], image_roots=image_roots, strip_prefixes=strip_prefixes)
        if not img_path:
            if skip_missing:
                continue
            raise FileNotFoundError(
                f"Could not find image for row {i}.\n"
                f"CSV value: {row[image_col]}\n"
                f"Tried roots: {list(image_roots)}\n"
                f"Tip: set image_roots to the directory that CONTAINS val2017/ (or contains data/coco/images/...)."
            )

        image = Image.open(img_path).convert("RGB")
        captions = [str(row[caption_col_template.format(j)]) for j in range(num_choices)]

        inputs = processor(
            text=captions,
            images=image,
            return_tensors="pt",
            padding=True,
            truncation=True,
        ).to(DEVICE)

        out = model(**inputs)
        pred = int(out.logits_per_image.argmax(dim=-1).item())
        label = _coerce_label(row[label_col], num_choices)

        preds.append(pred)
        labels.append(label)
        correct += (pred == label)
        used += 1

    acc = correct / used if used else 0.0
    return {"acc": acc, "correct": int(correct), "total": int(used), "preds": preds, "labels": labels}


def approx_95_ci(p, n):
    if n <= 0:
        return (float("nan"), float("nan"))
    se = math.sqrt(p * (1 - p) / n)
    lo, hi = p - 1.96 * se, p + 1.96 * se
    return (max(0.0, lo), min(1.0, hi))

ENG_CSV = "/content/COCO_val_mcq_llama3.1_rephrased.csv"
CHI_CSV = "/content/chinese.csv"

STRIP_PREFIXES = ("data/coco/images/", "/content/")

IMAGE_ROOTS = [
    ".",
    "/content",
    "/workspace",
    "/mnt/data",
]

eng = eval_mcq(ENG_CSV, image_roots=IMAGE_ROOTS, strip_prefixes=STRIP_PREFIXES)
chi = eval_mcq(CHI_CSV, image_roots=IMAGE_ROOTS, strip_prefixes=STRIP_PREFIXES)

print(f"English:  {eng['acc']:.4f} ({eng['correct']}/{eng['total']})  CI~{approx_95_ci(eng['acc'], eng['total'])}")
print(f"Chinese:  {chi['acc']:.4f} ({chi['correct']}/{chi['total']})  CI~{approx_95_ci(chi['acc'], chi['total'])}")

ARA_CSV = "/content/arabic.csv"
GRE_CSV = "/content/greek.csv"
RUS_CSV = "/content/russian.csv"
TAG_CSV = "/content/tagalog.csv"

STRIP_PREFIXES = ("data/coco/images/", "/content/")

IMAGE_ROOTS = [
    ".",
    "/content",
    "/workspace",
    "/mnt/data",
]

ara = eval_mcq(ARA_CSV, image_roots=IMAGE_ROOTS, strip_prefixes=STRIP_PREFIXES)
gre = eval_mcq(GRE_CSV, image_roots=IMAGE_ROOTS, strip_prefixes=STRIP_PREFIXES)
rus = eval_mcq(RUS_CSV, image_roots=IMAGE_ROOTS, strip_prefixes=STRIP_PREFIXES)
tag = eval_mcq(TAG_CSV, image_roots=IMAGE_ROOTS, strip_prefixes=STRIP_PREFIXES)

print(f"Arabic:  {ara['acc']:.4f} ({ara['correct']}/{ara['total']})  CI~{approx_95_ci(ara['acc'], ara['total'])}")
print(f"Greek:  {gre['acc']:.4f} ({gre['correct']}/{gre['total']})  CI~{approx_95_ci(gre['acc'], gre['total'])}")
print(f"Russian:  {rus['acc']:.4f} ({rus['correct']}/{rus['total']})  CI~{approx_95_ci(rus['acc'], rus['total'])}")
print(f"Tagalog:  {tag['acc']:.4f} ({tag['correct']}/{tag['total']})  CI~{approx_95_ci(tag['acc'], tag['total'])}")

ENG_CSV = "COCO_val_mcq_llama3.1_rephrased.csv"
SPA_CSV = "spanish.csv"

STRIP_PREFIXES = ("data/coco/images/", "/content/")

eng = eval_mcq(ENG_CSV, image_root=".", strip_prefixes=STRIP_PREFIXES)
spa = eval_mcq(SPA_CSV, image_root=".", strip_prefixes=STRIP_PREFIXES)

print(f"English:  {eng['acc']:.4f}  ({eng['correct']}/{eng['total']})  CI~{approx_95_ci(eng['acc'], eng['total'])}")
print(f"Spanish:  {spa['acc']:.4f}  ({spa['correct']}/{spa['total']})  CI~{approx_95_ci(spa['acc'], spa['total'])}")