# -*- coding: utf-8 -*-
"""spacevlm sol.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MSfF7H-wkpM1VaH5TvbVDEnSnacQK_-v
"""

!pip uninstall clip
!pip install git+https://github.com/openai/CLIP.git

import ast
import math
from typing import Callable, Optional, Tuple, Dict

import numpy as np
import clip
import pandas as pd
import torch
import torch.nn.functional as F
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from tqdm import tqdm

class MCQDataset(Dataset):
    def __init__(
        self,
        data_csv_path: str,
        caps_csv_path: str,
        image_transform: transforms.Compose,
    ):
        super().__init__()

        self.df = pd.read_csv(data_csv_path)
        self.caps = pd.read_csv(caps_csv_path)

        for col in ["caption_0", "caption_1", "caption_2", "caption_3"]:
          self.caps[col] = self.caps[col].apply(parse_caption_pair)

        assert len(self.df) == len(self.caps), "Data and captions length mismatch."
        self.transform = image_transform

    def __len__(self) -> int:
        return len(self.df)

    def _fix_affirmative(self, aff: str) -> str:
        return "A photo of something" if aff == "This is a photo" else aff

    def __getitem__(self, index: int):
        row = self.df.iloc[index]
        caps_row = self.caps.iloc[index]

        image_path = row["image_path"]
        image_path = "/content/drive/MyDrive/" + image_path[17:]
        image = Image.open(image_path).convert("RGB")
        image = self.transform(image)

        caption_0_aff, caption_0_neg = caps_row["caption_0"]
        caption_1_aff, caption_1_neg = caps_row["caption_1"]
        caption_2_aff, caption_2_neg = caps_row["caption_2"]
        caption_3_aff, caption_3_neg = caps_row["caption_3"]

        caption_0_aff = self._fix_affirmative(caption_0_aff)
        caption_1_aff = self._fix_affirmative(caption_1_aff)
        caption_2_aff = self._fix_affirmative(caption_2_aff)
        caption_3_aff = self._fix_affirmative(caption_3_aff)

        label = int(row["correct_answer"])  # 0..3
        template = row["correct_answer_template"]

        return (
            image,
            caption_0_aff,
            caption_0_neg,
            caption_1_aff,
            caption_1_neg,
            caption_2_aff,
            caption_2_neg,
            caption_3_aff,
            caption_3_neg,
            label,
            template,
        )

def build_coco_mcq_dataloader(
    data_csv_path: str,
    caps_csv_path: str,
    batch_size: int = 16,
    num_workers: int = 2,
    image_size: int = 224,
    base_preprocess: Optional[transforms.Compose] = None,
) -> DataLoader:

    if base_preprocess is None:
        tfm = transforms.Compose(
            [
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
            ]
        )
    else:
        tfm = transforms.Compose(
            [
                transforms.Resize(
                    (image_size, image_size),
                    transforms.InterpolationMode.BICUBIC,
                    antialias=True,
                )
            ]
            + list(base_preprocess.transforms)
        )

    dataset = MCQDataset(
        data_csv_path=data_csv_path,
        caps_csv_path=caps_csv_path,
        image_transform=tfm,
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )
    return dataloader

@torch.no_grad()
def text_embedding_compute(
    aff_caption,
    neg_caption,
    model,
    device: torch.device,
    threshold: float = 0.9,
    tokenize_fn: Optional[Callable[[list], torch.Tensor]] = None,
) -> torch.Tensor:
    if tokenize_fn is None:
        import clip
        tokenize_fn = clip.tokenize

    aff_np = np.array(aff_caption)
    neg_np = np.array(neg_caption)

    aff_mask = torch.from_numpy(aff_np != "A photo of something").to(device)
    neg_mask = torch.from_numpy(neg_np != "A photo of a ").to(device)

    only_aff_mask = aff_mask & ~neg_mask
    only_neg_mask = ~aff_mask & neg_mask
    hybrid_mask = aff_mask & neg_mask

    aff_embed = F.normalize(
        model.encode_text(tokenize_fn(aff_caption).to(device)), dim=-1
    )
    neg_embed = F.normalize(
        model.encode_text(tokenize_fn(neg_caption).to(device)), dim=-1
    )

    text_features = torch.zeros_like(aff_embed, device=device)
    text_features[only_aff_mask] = aff_embed[only_aff_mask]

    alpha = math.acos(threshold)
    a_T_n = torch.sum(aff_embed * neg_embed, dim=-1)
    theta = torch.acos(a_T_n.clamp(-1 + 1e-6, 1 - 1e-6))

    mask_pass = (theta < 2 * alpha) & (~only_aff_mask)
    mask_reject = (theta >= 2 * alpha) & (~only_aff_mask)

    delta = alpha + theta / 2.0
    hybrid_embed = (
        torch.cos(delta)[:, None] * neg_embed
        + (torch.sin(delta) / torch.sin(theta))[:, None]
        * (aff_embed - a_T_n[:, None] * neg_embed)
    )

    text_features[mask_pass] = hybrid_embed[mask_pass]
    text_features[mask_reject] = aff_embed[mask_reject]

    return text_features  # (B, D)

@torch.no_grad()
def evaluate_mcq(
    model,
    dataloader: DataLoader,
    device: torch.device,
    threshold: float = 0.92,
    tokenize_fn: Optional[Callable[[list], torch.Tensor]] = None,
    return_per_template: bool = True,
) -> Tuple[float, Optional[Dict[str, float]]]:
    model.eval()
    all_accs = []
    all_templates = []

    for (
        image,
        caption_0_aff,
        caption_0_neg,
        caption_1_aff,
        caption_1_neg,
        caption_2_aff,
        caption_2_neg,
        caption_3_aff,
        caption_3_neg,
        label,
        template,
    ) in tqdm(dataloader, total=len(dataloader), desc="Evaluating"):
        image = image.to(device, non_blocking=True)
        label = label.to(device, non_blocking=True)

        image_embed = F.normalize(model.encode_image(image), dim=-1)

        cap0 = text_embedding_compute(
            caption_0_aff, caption_0_neg, model, device, threshold, tokenize_fn
        )
        cap1 = text_embedding_compute(
            caption_1_aff, caption_1_neg, model, device, threshold, tokenize_fn
        )
        cap2 = text_embedding_compute(
            caption_2_aff, caption_2_neg, model, device, threshold, tokenize_fn
        )
        cap3 = text_embedding_compute(
            caption_3_aff, caption_3_neg, model, device, threshold, tokenize_fn
        )

        captions_embed = torch.stack([cap0, cap1, cap2, cap3], dim=-2)

        scores = torch.squeeze(
            image_embed[:, None, :] @ captions_embed.permute(0, 2, 1), dim=-2
        )
        preds = scores.argmax(dim=-1)

        batch_acc = (preds == label).float().cpu().numpy()
        all_accs.append(batch_acc)
        all_templates.extend(template)

    accs = np.concatenate(all_accs, axis=0)
    mean_acc = float(accs.mean())

    if not return_per_template:
        return mean_acc, None

    templates = np.array(all_templates)
    accs_per_template = {}
    for acc, tmpl in zip(accs, templates):
        accs_per_template.setdefault(tmpl, []).append(acc)
    accs_per_template = {k: float(np.mean(v)) for k, v in accs_per_template.items()}

    return mean_acc, accs_per_template

device = "cuda" if torch.cuda.is_available() else "cpu"

model, preprocess = clip.load("ViT-B/32", device=device, jit=False)

!wget -q http://images.cocodataset.org/zips/val2017.zip

!unzip -q val2017.zip

def parse_caption_pair(val):
    if isinstance(val, (list, tuple)) and len(val) == 2:
        return list(val)

    if not isinstance(val, str):
        return ["A photo of something", "A photo of a "]

    s = val.strip()
    if not s:
        return ["A photo of something", "A photo of a "]

    try:
        parsed = ast.literal_eval(s)
        if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
            return list(parsed)
    except Exception:
        pass

    trans_table = str.maketrans({
        "（": "(",
        "）": ")",
        "，": ",",
        "“": '"',
        "”": '"',
    })
    s_norm = s.translate(trans_table)
    try:
        parsed = ast.literal_eval(s_norm)
        if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
            return list(parsed)
    except Exception:
        pass

    parts = [p.strip(" ()[]\"' ") for p in s_norm.split(",")]
    parts = [p for p in parts if p]

    if len(parts) == 1:
        return [parts[0], "A photo of a "]
    elif len(parts) >= 2:
        return [parts[0], parts[1]]

    return ["A photo of something", "A photo of a "]

loaded_model_data = torch.load(
    "/content/drive/MyDrive/models/NegCLIP/negclip.pth",
    map_location="cpu",
    weights_only = False

)
missing, unexpected = model.load_state_dict(loaded_model_data, strict=False)
print("missing:", missing)
print("unexpected:", unexpected)

from google.colab import drive
drive.mount('/content/drive')

!cp "/content/drive/MyDrive/Copy of COCO_val_mcq_llama3.1_rephrased.csv" /content/data.csv
!cp "/content/drive/MyDrive/COCO_val_mcq_LLM_captions (1).csv" /content/caps.csv

dataloader = build_coco_mcq_dataloader(
    data_csv_path="/content/data.csv",
    caps_csv_path="/content/caps.csv",
    batch_size=128,
    num_workers=0,
    base_preprocess=preprocess,
)

def parse_caption_pair(val):
    if isinstance(val, (list, tuple)) and len(val) == 2:
        return list(val)

    if not isinstance(val, str):
        return ["A photo of something", "A photo of a "]

    s = val.strip()
    if not s:
        return ["A photo of something", "A photo of a "]

    try:
        parsed = ast.literal_eval(s)
        if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
            return list(parsed)
    except Exception:
        pass

    trans_table = str.maketrans({
        "（": "(",
        "）": ")",
        "，": ",",
        "“": '"',
        "”": '"',
    })
    s_norm = s.translate(trans_table)
    try:
        parsed = ast.literal_eval(s_norm)
        if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
            return list(parsed)
    except Exception:
        pass

    parts = [p.strip(" ()[]\"' ") for p in s_norm.split(",")]
    parts = [p for p in parts if p]

    if len(parts) == 1:
        return [parts[0], "A photo of a "]
    elif len(parts) >= 2:
        return [parts[0], parts[1]]
    return ["A photo of something", "A photo of a "]

dataloader = build_coco_mcq_dataloader(
    data_csv_path="/content/drive/MyDrive/lang csvs/COCO_val_mcq_llama3.1_rephrased.csv",
    caps_csv_path="/content/drive/MyDrive/COCO_val_mcq_LLM_captions (1).csv",

    batch_size=1,
    num_workers=0,
    base_preprocess=preprocess,
)

mean_acc, accs_per_template = evaluate_mcq(
    model,
    dataloader,
    device=torch.device(device),
    threshold=0.92,
    tokenize_fn=lambda x: clip.tokenize(x, context_length=77, truncate=True),
)

print("Mean accuracy:", mean_acc)
print("Per-template:", accs_per_template)

!pip -q install -U transformers sentence-transformers accelerate ftfy regex tqdm pandas pillow

import os, glob, math, ast
import numpy as np
import pandas as pd
from tqdm import tqdm
from PIL import Image

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import CLIPModel, AutoProcessor
from sentence_transformers import SentenceTransformer

BASE_DIR = "/content/drive/MyDrive/lang csvs"
DATA_GLOB = os.path.join(BASE_DIR, "COCO_val_mcq_llama3.1_rephrased*.csv")
CAPS_GLOB = os.path.join(BASE_DIR, "COCO_val_mcq_LLM_captions*.csv")

BATCH_SIZE = 64
NUM_WORKERS = 0
THRESHOLD = 0.92
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

from google.colab import drive
drive.mount("/content/drive")

def infer_lang_from_path(p: str) -> str:
    base = os.path.basename(p)
    if "__" in base:
        return base.rsplit("__", 1)[1].split(".")[0]
    return "en"

def parse_caption_pair(val):
    if isinstance(val, (list, tuple)) and len(val) == 2:
        return [str(val[0]), str(val[1])]

    if not isinstance(val, str):
        return ["A photo of something", "A photo of a "]

    s = val.strip()
    if not s:
        return ["A photo of something", "A photo of a "]

    try:
        parsed = ast.literal_eval(s)
        if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
            return [str(parsed[0]), str(parsed[1])]
    except Exception:
        pass
    trans = str.maketrans({"（":"(", "）":")", "，":",", "“":'"', "”":'"'})
    s2 = s.translate(trans)
    try:
        parsed = ast.literal_eval(s2)
        if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
            return [str(parsed[0]), str(parsed[1])]
    except Exception:
        pass

    parts = [p.strip(" ()[]\"' ") for p in s2.split(",")]
    parts = [p for p in parts if p]
    if len(parts) == 1:
        return [parts[0], "A photo of a "]
    if len(parts) >= 2:
        return [parts[0], parts[1]]
    return ["A photo of something", "A photo of a "]

def fix_affirmative(aff: str) -> str:
    return "A photo of something" if aff == "This is a photo" else aff

def resolve_image_path(p: str) -> str:
    p = str(p)
    if p.startswith("/content/drive/"):
        return p
    if p.startswith("MyDrive/"):
        return "/content/drive/" + p
    return os.path.join("/content/drive/MyDrive", p.lstrip("/"))

class MCQDataset(Dataset):
    def __init__(self, data_csv_path: str, caps_csv_path: str):
        self.df = pd.read_csv(data_csv_path)
        self.caps = pd.read_csv(caps_csv_path)
        for col in ["caption_0","caption_1","caption_2","caption_3"]:
            self.caps[col] = self.caps[col].apply(parse_caption_pair)
        assert len(self.df) == len(self.caps), "Data and captions length mismatch."

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx: int):
        row = self.df.iloc[idx]
        caps_row = self.caps.iloc[idx]

        image_path = resolve_image_path(row["image_path"])
        image = Image.open(image_path).convert("RGB")

        c0_aff, c0_neg = caps_row["caption_0"]
        c1_aff, c1_neg = caps_row["caption_1"]
        c2_aff, c2_neg = caps_row["caption_2"]
        c3_aff, c3_neg = caps_row["caption_3"]

        c0_aff = fix_affirmative(c0_aff)
        c1_aff = fix_affirmative(c1_aff)
        c2_aff = fix_affirmative(c2_aff)
        c3_aff = fix_affirmative(c3_aff)

        label = int(row["correct_answer"])  # 0..3
        template = row["correct_answer_template"]

        return (
            image,
            c0_aff, c0_neg,
            c1_aff, c1_neg,
            c2_aff, c2_neg,
            c3_aff, c3_neg,
            label,
            template,
        )

def mcq_collate(batch):
    images = [b[0] for b in batch]
    cols = list(zip(*[b[1:] for b in batch]))
    labels = torch.tensor(cols[-2], dtype=torch.long)
    templates = list(cols[-1])
    rest = [list(c) for c in cols[:-2]]
    return (images, *rest, labels, templates)

@torch.no_grad()
def hybrid_text_features(aff_texts, neg_texts, text_encoder: SentenceTransformer, threshold: float):
    aff_np = np.array(aff_texts)
    neg_np = np.array(neg_texts)
    aff_mask = torch.from_numpy(aff_np != "A photo of something").to(DEVICE)
    neg_mask = torch.from_numpy(neg_np != "A photo of a ").to(DEVICE)
    only_aff_mask = aff_mask & ~neg_mask

    aff_emb = text_encoder.encode(
        aff_texts, convert_to_tensor=True, device=DEVICE, normalize_embeddings=True
    )
    neg_emb = text_encoder.encode(
        neg_texts, convert_to_tensor=True, device=DEVICE, normalize_embeddings=True
    )

    text_feat = torch.zeros_like(aff_emb)

    text_feat[only_aff_mask] = aff_emb[only_aff_mask]

    alpha = math.acos(threshold)
    a_T_n = torch.sum(aff_emb * neg_emb, dim=-1)
    theta = torch.acos(a_T_n.clamp(-1 + 1e-6, 1 - 1e-6))

    mask_pass = (theta < 2 * alpha) & (~only_aff_mask)
    mask_reject = (theta >= 2 * alpha) & (~only_aff_mask)

    delta = alpha + theta / 2.0
    hybrid = (
        torch.cos(delta)[:, None] * neg_emb
        + (torch.sin(delta) / torch.sin(theta)).clamp(min=0)[:, None]
        * (aff_emb - a_T_n[:, None] * neg_emb)
    )

    text_feat[mask_pass] = hybrid[mask_pass]
    text_feat[mask_reject] = aff_emb[mask_reject]

    return F.normalize(text_feat, dim=-1)

@torch.no_grad()
def evaluate_one_language(data_csv, caps_csv, text_encoder, clip_vision, clip_processor, threshold=0.92):
    ds = MCQDataset(data_csv, caps_csv)
    dl = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, collate_fn=mcq_collate)

    all_acc = []
    all_templates = []

    for (
        images,
        c0_aff, c0_neg,
        c1_aff, c1_neg,
        c2_aff, c2_neg,
        c3_aff, c3_neg,
        labels,
        templates
    ) in tqdm(dl, total=len(dl), desc=f"Evaluating {os.path.basename(caps_csv)}"):

        labels = labels.to(DEVICE)

        img_inputs = clip_processor(images=images, return_tensors="pt").to(DEVICE)
        img_feat = clip_vision.get_image_features(**img_inputs)
        img_feat = F.normalize(img_feat, dim=-1)  # (B, D)

        t0 = hybrid_text_features(c0_aff, c0_neg, text_encoder, threshold)
        t1 = hybrid_text_features(c1_aff, c1_neg, text_encoder, threshold)
        t2 = hybrid_text_features(c2_aff, c2_neg, text_encoder, threshold)
        t3 = hybrid_text_features(c3_aff, c3_neg, text_encoder, threshold)

        caps_feat = torch.stack([t0, t1, t2, t3], dim=1)  # (B, 4, D)
        scores = torch.sum(img_feat[:, None, :] * caps_feat, dim=-1)  # (B, 4)
        preds = scores.argmax(dim=-1)

        acc = (preds == labels).float().cpu().numpy()
        all_acc.append(acc)
        all_templates.extend(templates)

    accs = np.concatenate(all_acc, axis=0)
    mean_acc = float(accs.mean())

    tmpl = np.array(all_templates)
    per_t = {}
    for a, t in zip(accs, tmpl):
        per_t.setdefault(t, []).append(a)
    per_t = {k: float(np.mean(v)) for k, v in per_t.items()}

    return mean_acc, per_t

TEXT_MODEL_ID = "sentence-transformers/clip-ViT-B-32-multilingual-v1"
text_encoder = SentenceTransformer(TEXT_MODEL_ID, device=DEVICE)
text_dim = text_encoder.get_sentence_embedding_dimension()

VISION_MODEL_ID = "openai/clip-vit-base-patch32"
clip_processor = AutoProcessor.from_pretrained(VISION_MODEL_ID)
clip_vision = CLIPModel.from_pretrained(VISION_MODEL_ID).to(DEVICE).eval()
vision_dim = clip_vision.config.projection_dim

print("Text dim:", text_dim, "| Vision dim:", vision_dim)
assert text_dim == vision_dim, "Dim mismatch: choose a matching CLIP vision backbone."

data_paths = sorted(glob.glob(DATA_GLOB))
caps_paths = sorted(glob.glob(CAPS_GLOB))

if len(caps_paths) == 0:
    raise FileNotFoundError(f"No caption CSVs matched: {CAPS_GLOB}\nCheck BASE_DIR and filename pattern.")
if len(data_paths) == 0:
    raise FileNotFoundError(f"No data CSVs matched: {DATA_GLOB}\nCheck BASE_DIR and filename pattern.")

data_by_lang = {infer_lang_from_path(p): p for p in data_paths}
caps_by_lang = {infer_lang_from_path(p): p for p in caps_paths}

results = {}
for lang, caps_csv in caps_by_lang.items():
    data_csv = data_by_lang.get(lang, data_by_lang.get("en"))
    print("\n" + "="*80)
    print(f"LANG = {lang}")
    print(f"DATA = {data_csv}")
    print(f"CAPS = {caps_csv}")
    print("="*80)

    mean_acc, per_t = evaluate_one_language(
        data_csv=data_csv,
        caps_csv=caps_csv,
        text_encoder=text_encoder,
        clip_vision=clip_vision,
        clip_processor=clip_processor,
        threshold=THRESHOLD
    )
    results[lang] = {"mean_acc": mean_acc, "per_template": per_t}
    print(f"Mean accuracy ({lang}): {mean_acc:.4f}")

for lang in sorted(results.keys()):
    print(f"{lang}: {results[lang]['mean_acc']:.4f}")

