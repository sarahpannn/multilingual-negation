# -*- coding: utf-8 -*-
"""spaceVLM-multiClip.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q6C8nBBD3EiFmxGPUji8dIWGKprOiWZQ
"""

!pip -q uninstall -y pillow
!pip -q install "pillow==11.0.0" "pandas==2.2.2"

import PIL, pandas

!pip -q install transformers sentence-transformers accelerate ftfy regex tqdm

# MultiCLIP + SpaceVLM eval (MCQ) across languages

import os, glob, ast, math
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm

import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from transformers import CLIPModel, AutoProcessor
from sentence_transformers import SentenceTransformer

from google.colab import drive
drive.mount("/content/drive")

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DATA_CSV = "/content/drive/MyDrive/lang csvs/COCO_val_mcq_llama3.1_rephrased.csv"
COCO_IMAGES_ROOT = "/content/drive/MyDrive"

CAPS_DIR = "/content/drive/MyDrive/new lang csvs"
CAPS_FILES = sorted(glob.glob(os.path.join(CAPS_DIR, "COCO_val_mcq_LLM_captions*.csv")))
TEXT_MODEL_ID = "sentence-transformers/clip-ViT-B-32-multilingual-v1"
VISION_MODEL_ID = "openai/clip-vit-base-patch32"

text_encoder = SentenceTransformer(TEXT_MODEL_ID, device=DEVICE)
clip_processor = AutoProcessor.from_pretrained(VISION_MODEL_ID)
clip_vision = CLIPModel.from_pretrained(VISION_MODEL_ID).to(DEVICE).eval()

def approx_95_ci(p, n):
    if n <= 0: return (float("nan"), float("nan"))
    se = math.sqrt(max(p*(1-p), 1e-12) / n)
    return (p - 1.96*se, p + 1.96*se)

def infer_lang(file_name: str):
    base = os.path.basename(file_name)
    if "__" in base:
        return base.split("__", 1)[1].rsplit(".", 1)[0]
    return "en"

def resolve_image_path(p: str):
    p = str(p).strip()
    if os.path.isabs(p) and os.path.exists(p):
        return p
    p = p.lstrip("./")

    STRIP_PREFIXES = [
        "data/coco/images/",
        "data/coco/",
        "coco/images/",
        "coco/",
        "/content/",
        "content/",
    ]
    for pref in STRIP_PREFIXES:
        if p.startswith(pref):
            p = p[len(pref):]

    cand = os.path.join(COCO_IMAGES_ROOT, p)
    if os.path.exists(cand):
        return cand

    base = os.path.basename(p)
    cand2 = os.path.join(COCO_IMAGES_ROOT, "val2017", base)
    if os.path.exists(cand2):
        return cand2

def parse_pair(cell):
    if pd.isna(cell): return ("A photo of something", "A photo of a ")
    if isinstance(cell, (list, tuple)) and len(cell) == 2:
        return (str(cell[0]), str(cell[1]))
    s = str(cell).strip()
    try:
        v = ast.literal_eval(s)
        if isinstance(v, (list, tuple)) and len(v) == 2:
            return (str(v[0]), str(v[1]))
    except Exception:
        pass
    return (s, "A photo of a ")

@torch.inference_mode()
def encode_text(texts):
    emb = text_encoder.encode(
        texts,
        convert_to_tensor=True,
        device=DEVICE,
        normalize_embeddings=True,
        show_progress_bar=False,
    )
    return F.normalize(emb, dim=-1)

@torch.inference_mode()
def encode_images(pil_images):
    inp = clip_processor(images=pil_images, return_tensors="pt").to(DEVICE)
    emb = clip_vision.get_image_features(**inp)
    return F.normalize(emb, dim=-1)

@torch.inference_mode()
def spacevlm_hybrid_affneg(aff_texts, neg_texts, threshold=0.92):
    ea = encode_text(aff_texts)
    en = encode_text(neg_texts)

    alpha = math.acos(threshold)
    aff_mask = torch.tensor([a != "A photo of something" for a in aff_texts], device=DEVICE)
    neg_mask = torch.tensor([n != "A photo of a "        for n in neg_texts], device=DEVICE)
    only_aff = aff_mask & (~neg_mask)
    both = aff_mask & neg_mask

    out = torch.zeros_like(ea)
    out[only_aff] = ea[only_aff]

    aTn = (ea * en).sum(dim=-1).clamp(-1 + 1e-6, 1 - 1e-6)
    theta = torch.acos(aTn)
    pass_mask = (theta < 2 * alpha) & both
    reject_mask = (theta >= 2 * alpha) & both

    # hybrid formula
    denom = torch.sin(theta).clamp_min(1e-6)
    delta = alpha + theta / 2.0
    hybrid = (
        torch.cos(delta)[:, None] * en
        + (torch.sin(delta) / denom)[:, None] * (ea - aTn[:, None] * en)
    )
    hybrid = F.normalize(hybrid, dim=-1)

    out[pass_mask] = hybrid[pass_mask]
    out[reject_mask] = ea[reject_mask]

    # any leftovers: just aff
    leftover = ~(only_aff | both)
    out[leftover] = ea[leftover]
    return F.normalize(out, dim=-1)

def eval_spacevlm_on_caps(data_csv, caps_csv, batch_size=64, threshold=0.92):
    df = pd.read_csv(data_csv)
    caps = pd.read_csv(caps_csv)
    assert len(df) == len(caps), f"Row mismatch data={len(df)} caps={len(caps)}"
    for j in range(4):
        caps[f"caption_{j}"] = caps[f"caption_{j}"].apply(parse_pair)

    correct = 0
    n = len(df)

    for start in tqdm(range(0, n, batch_size), desc=f"{infer_lang(caps_csv)}"):
        end = min(n, start + batch_size)
        rows = df.iloc[start:end]
        caprows = caps.iloc[start:end]

        images = [Image.open(resolve_image_path(p)).convert("RGB") for p in rows["image_path"].tolist()]
        img_feat = encode_images(images)  # (B,D)

        option_feats = []
        for j in range(4):
            affs = [caprows.iloc[i][f"caption_{j}"][0] for i in range(len(caprows))]
            negs = [caprows.iloc[i][f"caption_{j}"][1] for i in range(len(caprows))]
            txt = spacevlm_hybrid_affneg(affs, negs, threshold=threshold)  # (B,D)
            option_feats.append(txt)

        txt_feat = torch.stack(option_feats, dim=1)  # (B,4,D)
        sims = torch.einsum("bd,bkd->bk", img_feat, txt_feat)  # (B,4)
        preds = sims.argmax(dim=1).cpu().numpy()

        labels = rows["correct_answer"].astype(int).to_numpy()
        correct += int((preds == labels).sum())

    acc = correct / n
    return acc, correct, n, approx_95_ci(acc, n)

for caps_csv in CAPS_FILES:
    lang = infer_lang(caps_csv)
    acc, corr, tot, ci = eval_spacevlm_on_caps(DATA_CSV, caps_csv, batch_size=64, threshold=0.92)
    print(f"{lang}: {acc:.4f} ({corr}/{tot})  CI~{ci}")

# SigLIP + SpaceVLM MCQ evaluation across languages

!pip -q install "pillow==11.0.0" "pandas==2.2.2" transformers accelerate ftfy regex tqdm

import os, glob, ast, math
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm

import torch
import torch.nn.functional as F
from transformers import AutoModel, AutoProcessor

from google.colab import drive
drive.mount("/content/drive")

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DATA_CSV = "/content/drive/MyDrive/lang csvs/COCO_val_mcq_llama3.1_rephrased.csv"
COCO_IMAGES_ROOT = "/content/drive/MyDrive"
PAIR_CAPS_DIR = "/content/drive/MyDrive/new lang csvs"

THRESHOLD = 0.92
BATCH_SIZE = 64

SIGLIP_MODEL_ID = "google/siglip-base-patch16-256-multilingual"
siglip_model = AutoModel.from_pretrained(SIGLIP_MODEL_ID).to(DEVICE).eval()
siglip_processor = AutoProcessor.from_pretrained(SIGLIP_MODEL_ID)

def approx_95_ci(p, n):
    if n <= 0: return (float("nan"), float("nan"))
    se = math.sqrt(max(p*(1-p), 1e-12) / n)
    return (max(0.0, p - 1.96*se), min(1.0, p + 1.96*se))

def infer_lang(file_name: str):
    base = os.path.basename(file_name)
    if "__" in base:
        return base.split("__", 1)[1].rsplit(".", 1)[0]
    return "en"

def resolve_image_path(p: str):
    p0 = str(p).strip()
    if os.path.isabs(p0) and os.path.exists(p0):
        return p0

    p = p0.lstrip("./")
    for pref in ["data/coco/images/", "data/coco/", "coco/images/", "coco/", "/content/", "content/"]:
        if p.startswith(pref):
            p = p[len(pref):]

    cands = [
        os.path.join(COCO_IMAGES_ROOT, p),
        os.path.join(COCO_IMAGES_ROOT, "images", p),
        os.path.join(COCO_IMAGES_ROOT, "val2017", os.path.basename(p)),
        os.path.join(COCO_IMAGES_ROOT, "images", "val2017", os.path.basename(p)),
    ]
    for c in cands:
        if os.path.exists(c):
            return c
    raise FileNotFoundError(f"Could not resolve image_path: {p0}")

def parse_pair(cell):
    if isinstance(cell, (list, tuple)) and len(cell) == 2:
        return (str(cell[0]), str(cell[1]))
    if cell is None or (isinstance(cell, float) and np.isnan(cell)):
        return ("A photo of something", "A photo of a ")
    s = str(cell).strip()
    try:
        v = ast.literal_eval(s)
        if isinstance(v, (list, tuple)) and len(v) == 2:
            return (str(v[0]), str(v[1]))
    except Exception:
        pass
    return (s, "A photo of a ")

@torch.inference_mode()
def encode_text(texts):
    inputs = siglip_processor(text=list(texts), padding=True, truncation=True, return_tensors="pt")
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    if hasattr(siglip_model, "get_text_features"):
        emb = siglip_model.get_text_features(**inputs)
    else:
        out = siglip_model(**inputs)
        emb = getattr(out, "text_embeds", None)
    return F.normalize(emb, dim=-1)

@torch.inference_mode()
def encode_images(pil_images):
    inputs = siglip_processor(images=list(pil_images), return_tensors="pt")
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    if hasattr(siglip_model, "get_image_features"):
        emb = siglip_model.get_image_features(**inputs)
    else:
        out = siglip_model(**inputs)
        emb = getattr(out, "image_embeds", None)
    return F.normalize(emb, dim=-1)

@torch.inference_mode()
def spacevlm_hybrid_affneg(aff_texts, neg_texts, threshold=0.92):
    ea = encode_text(aff_texts)
    en = encode_text(neg_texts)

    alpha = math.acos(threshold)

    aff_mask = torch.tensor([a != "A photo of something" for a in aff_texts], device=DEVICE)
    neg_mask = torch.tensor([n != "A photo of a "        for n in neg_texts], device=DEVICE)

    only_aff = aff_mask & (~neg_mask)
    both = aff_mask & neg_mask

    out = torch.zeros_like(ea)
    out[only_aff] = ea[only_aff]

    aTn = (ea * en).sum(dim=-1).clamp(-1 + 1e-6, 1 - 1e-6)
    theta = torch.acos(aTn)

    pass_mask = (theta < 2 * alpha) & both
    reject_mask = (theta >= 2 * alpha) & both

    denom = torch.sin(theta).clamp_min(1e-6)
    delta = alpha + theta / 2.0

    hybrid = (
        torch.cos(delta)[:, None] * en
        + (torch.sin(delta) / denom)[:, None] * (ea - aTn[:, None] * en)
    )
    hybrid = F.normalize(hybrid, dim=-1)

    out[pass_mask] = hybrid[pass_mask]
    out[reject_mask] = ea[reject_mask]

    leftover = ~(only_aff | both)
    out[leftover] = ea[leftover]
    return F.normalize(out, dim=-1)

def eval_spacevlm_one_language(data_csv, pair_caps_csv):
    df = pd.read_csv(data_csv)
    caps = pd.read_csv(pair_caps_csv)
    assert len(df) == len(caps), f"Row mismatch data={len(df)} caps={len(caps)}"

    for j in range(4):
        caps[f"caption_{j}"] = caps[f"caption_{j}"].apply(parse_pair)

    correct = 0
    n = len(df)

    for start in tqdm(range(0, n, BATCH_SIZE), desc=infer_lang(pair_caps_csv)):
        end = min(n, start + BATCH_SIZE)
        rows = df.iloc[start:end]
        caprows = caps.iloc[start:end]

        images = [Image.open(resolve_image_path(p)).convert("RGB") for p in rows["image_path"].tolist()]
        img_feat = encode_images(images)

        option_feats = []
        for j in range(4):
            affs = [caprows.iloc[i][f"caption_{j}"][0] for i in range(len(caprows))]
            negs = [caprows.iloc[i][f"caption_{j}"][1] for i in range(len(caprows))]
            option_feats.append(spacevlm_hybrid_affneg(affs, negs, threshold=THRESHOLD))

        txt_feat = torch.stack(option_feats, dim=1)  # (B,4,D)
        sims = torch.einsum("bd,bkd->bk", img_feat, txt_feat)
        preds = sims.argmax(dim=1).cpu().numpy()

        labels = rows["correct_answer"].astype(int).to_numpy()
        correct += int((preds == labels).sum())

    acc = correct / n
    return acc, correct, n, approx_95_ci(acc, n)
pair_files = sorted(glob.glob(os.path.join(PAIR_CAPS_DIR, "COCO_val_mcq_LLM_captions*.csv")))

for pcsv in pair_files:
    lang = infer_lang(pcsv)
    acc, corr, tot, ci = eval_spacevlm_one_language(DATA_CSV, pcsv)
    print(f"{lang}: {acc:.4f} ({corr}/{tot})  CI~{ci}")

# CLIP (openai/clip-vit-base-patch32) + SpaceVLM

!pip -q install "pillow==11.0.0" "pandas==2.2.2" transformers accelerate ftfy regex tqdm

import os, glob, ast, math
import numpy as np
import pandas as pd
from PIL import Image
from tqdm import tqdm

import torch
import torch.nn.functional as F
from transformers import CLIPModel, CLIPProcessor

from google.colab import drive
drive.mount("/content/drive")

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DATA_CSV = "/content/drive/MyDrive/lang csvs/COCO_val_mcq_llama3.1_rephrased.csv"
COCO_IMAGES_ROOT = "/content/drive/MyDrive"
PAIR_CAPS_DIR = "/content/drive/MyDrive/new lang csvs"

THRESHOLD = 0.92
BATCH_SIZE = 64

CLIP_MODEL_ID = "openai/clip-vit-base-patch32"
clip_model = CLIPModel.from_pretrained(CLIP_MODEL_ID).to(DEVICE).eval()
clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_ID)

def approx_95_ci(p, n):
    if n <= 0: return (float("nan"), float("nan"))
    se = math.sqrt(max(p*(1-p), 1e-12) / n)
    return (max(0.0, p - 1.96*se), min(1.0, p + 1.96*se))

def infer_lang(file_name: str):
    base = os.path.basename(file_name)
    if "__" in base:
        return base.split("__", 1)[1].rsplit(".", 1)[0]
    return "en"  # e.g. " (1).csv"

def resolve_image_path(p: str):
    p0 = str(p).strip()
    if os.path.isabs(p0) and os.path.exists(p0):
        return p0

    p = p0.lstrip("./")
    for pref in ["data/coco/images/", "data/coco/", "coco/images/", "coco/", "/content/", "content/"]:
        if p.startswith(pref):
            p = p[len(pref):]

    cands = [
        os.path.join(COCO_IMAGES_ROOT, p),
        os.path.join(COCO_IMAGES_ROOT, "images", p),
        os.path.join(COCO_IMAGES_ROOT, "val2017", os.path.basename(p)),
        os.path.join(COCO_IMAGES_ROOT, "images", "val2017", os.path.basename(p)),
    ]
    for c in cands:
        if os.path.exists(c):
            return c
    raise FileNotFoundError(f"Could not resolve image_path: {p0}")

def parse_pair(cell):
    if isinstance(cell, (list, tuple)) and len(cell) == 2:
        return (str(cell[0]), str(cell[1]))
    if cell is None or (isinstance(cell, float) and np.isnan(cell)):
        return ("A photo of something", "A photo of a ")
    s = str(cell).strip()
    try:
        v = ast.literal_eval(s)
        if isinstance(v, (list, tuple)) and len(v) == 2:
            return (str(v[0]), str(v[1]))
    except Exception:
        pass
    return (s, "A photo of a ")

@torch.inference_mode()
def encode_text(texts):
    inputs = clip_processor(text=list(texts), padding=True, truncation=True, return_tensors="pt")
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    emb = clip_model.get_text_features(**inputs)
    return F.normalize(emb, dim=-1)

@torch.inference_mode()
def encode_images(pil_images):
    inputs = clip_processor(images=list(pil_images), return_tensors="pt")
    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
    emb = clip_model.get_image_features(**inputs)
    return F.normalize(emb, dim=-1)

@torch.inference_mode()
def spacevlm_hybrid_affneg(aff_texts, neg_texts, threshold=0.92):
    ea = encode_text(aff_texts)
    en = encode_text(neg_texts)

    alpha = math.acos(threshold)

    aff_mask = torch.tensor([a != "A photo of something" for a in aff_texts], device=DEVICE)
    neg_mask = torch.tensor([n != "A photo of a "        for n in neg_texts], device=DEVICE)

    only_aff = aff_mask & (~neg_mask)
    both = aff_mask & neg_mask

    out = torch.zeros_like(ea)
    out[only_aff] = ea[only_aff]

    aTn = (ea * en).sum(dim=-1).clamp(-1 + 1e-6, 1 - 1e-6)
    theta = torch.acos(aTn)

    pass_mask = (theta < 2 * alpha) & both
    reject_mask = (theta >= 2 * alpha) & both

    denom = torch.sin(theta).clamp_min(1e-6)
    delta = alpha + theta / 2.0

    hybrid = (
        torch.cos(delta)[:, None] * en
        + (torch.sin(delta) / denom)[:, None] * (ea - aTn[:, None] * en)
    )
    hybrid = F.normalize(hybrid, dim=-1)

    out[pass_mask] = hybrid[pass_mask]
    out[reject_mask] = ea[reject_mask]

    leftover = ~(only_aff | both)
    out[leftover] = ea[leftover]
    return F.normalize(out, dim=-1)

def eval_spacevlm_one_language(data_csv, pair_caps_csv):
    df = pd.read_csv(data_csv)
    caps = pd.read_csv(pair_caps_csv)
    assert len(df) == len(caps), f"Row mismatch data={len(df)} caps={len(caps)}"

    for j in range(4):
        caps[f"caption_{j}"] = caps[f"caption_{j}"].apply(parse_pair)

    correct = 0
    n = len(df)

    for start in tqdm(range(0, n, BATCH_SIZE), desc=infer_lang(pair_caps_csv)):
        end = min(n, start + BATCH_SIZE)
        rows = df.iloc[start:end]
        caprows = caps.iloc[start:end]

        images = [Image.open(resolve_image_path(p)).convert("RGB") for p in rows["image_path"].tolist()]
        img_feat = encode_images(images)  # (B,D)

        option_feats = []
        for j in range(4):
            affs = [caprows.iloc[i][f"caption_{j}"][0] for i in range(len(caprows))]
            negs = [caprows.iloc[i][f"caption_{j}"][1] for i in range(len(caprows))]
            option_feats.append(spacevlm_hybrid_affneg(affs, negs, threshold=THRESHOLD))

        txt_feat = torch.stack(option_feats, dim=1)  # (B,4,D)
        sims = torch.einsum("bd,bkd->bk", img_feat, txt_feat)
        preds = sims.argmax(dim=1).cpu().numpy()

        labels = rows["correct_answer"].astype(int).to_numpy()
        correct += int((preds == labels).sum())

    acc = correct / n
    return acc, correct, n, approx_95_ci(acc, n)

pair_files = sorted(glob.glob(os.path.join(PAIR_CAPS_DIR, "COCO_val_mcq_LLM_captions*.csv")))

for pcsv in pair_files:
    lang = infer_lang(pcsv)
    acc, corr, tot, ci = eval_spacevlm_one_language(DATA_CSV, pcsv)
    print(f"{lang}: {acc:.4f} ({corr}/{tot})  CI~{ci}")