# -*- coding: utf-8 -*-
"""spacevlm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g2Ai4DwSPuae8ce6Vx01HrWtK_RVWd0e
"""

!pip uninstall clip
!pip install git+https://github.com/openai/CLIP.git

import ast
import math
from typing import Callable, Optional, Tuple, Dict

import numpy as np
import clip
import pandas as pd
import torch
import torch.nn.functional as F
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from tqdm import tqdm

class MCQDataset(Dataset):
    def __init__(
        self,
        data_csv_path: str,
        caps_csv_path: str,
        image_transform: transforms.Compose,
    ):
        super().__init__()

        self.df = pd.read_csv(data_csv_path)
        self.caps = pd.read_csv(caps_csv_path)

        for col in ["caption_0", "caption_1", "caption_2", "caption_3"]:
          self.caps[col] = self.caps[col].apply(parse_caption_pair)

        assert len(self.df) == len(self.caps), "Data and captions length mismatch."
        self.transform = image_transform

    def __len__(self) -> int:
        return len(self.df)

    def _fix_affirmative(self, aff: str) -> str:
        return "A photo of something" if aff == "This is a photo" else aff

    def __getitem__(self, index: int):
        row = self.df.iloc[index]
        caps_row = self.caps.iloc[index]

        image_path = row["image_path"]
        image_path = "/content/drive/MyDrive/" + image_path[17:]
        image = Image.open(image_path).convert("RGB")
        image = self.transform(image)

        caption_0_aff, caption_0_neg = caps_row["caption_0"]
        caption_1_aff, caption_1_neg = caps_row["caption_1"]
        caption_2_aff, caption_2_neg = caps_row["caption_2"]
        caption_3_aff, caption_3_neg = caps_row["caption_3"]

        caption_0_aff = self._fix_affirmative(caption_0_aff)
        caption_1_aff = self._fix_affirmative(caption_1_aff)
        caption_2_aff = self._fix_affirmative(caption_2_aff)
        caption_3_aff = self._fix_affirmative(caption_3_aff)

        label = int(row["correct_answer"])  # 0..3
        template = row["correct_answer_template"]

        return (
            image,
            caption_0_aff,
            caption_0_neg,
            caption_1_aff,
            caption_1_neg,
            caption_2_aff,
            caption_2_neg,
            caption_3_aff,
            caption_3_neg,
            label,
            template,
        )

def build_coco_mcq_dataloader(
    data_csv_path: str,
    caps_csv_path: str,
    batch_size: int = 16,
    num_workers: int = 2,
    image_size: int = 224,
    base_preprocess: Optional[transforms.Compose] = None,
) -> DataLoader:
    if base_preprocess is None:
        tfm = transforms.Compose(
            [
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
            ]
        )
    else:
        tfm = transforms.Compose(
            [
                transforms.Resize(
                    (image_size, image_size),
                    transforms.InterpolationMode.BICUBIC,
                    antialias=True,
                )
            ]
            + list(base_preprocess.transforms)
        )

    dataset = MCQDataset(
        data_csv_path=data_csv_path,
        caps_csv_path=caps_csv_path,
        image_transform=tfm,
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
    )
    return dataloader

@torch.no_grad()
def text_embedding_compute(
    aff_caption,
    neg_caption,
    model,
    device: torch.device,
    threshold: float = 0.9,
    tokenize_fn: Optional[Callable[[list], torch.Tensor]] = None,
) -> torch.Tensor:
    if tokenize_fn is None:
        import clip
        tokenize_fn = clip.tokenize

    aff_np = np.array(aff_caption)
    neg_np = np.array(neg_caption)

    aff_mask = torch.from_numpy(aff_np != "A photo of something").to(device)
    neg_mask = torch.from_numpy(neg_np != "A photo of a ").to(device)

    only_aff_mask = aff_mask & ~neg_mask
    only_neg_mask = ~aff_mask & neg_mask
    hybrid_mask = aff_mask & neg_mask

    aff_embed = F.normalize(
        model.encode_text(tokenize_fn(aff_caption).to(device)), dim=-1
    )
    neg_embed = F.normalize(
        model.encode_text(tokenize_fn(neg_caption).to(device)), dim=-1
    )

    text_features = torch.zeros_like(aff_embed, device=device)

    text_features[only_aff_mask] = aff_embed[only_aff_mask]

    alpha = math.acos(threshold)
    a_T_n = torch.sum(aff_embed * neg_embed, dim=-1)
    theta = torch.acos(a_T_n.clamp(-1 + 1e-6, 1 - 1e-6))

    mask_pass = (theta < 2 * alpha) & (~only_aff_mask)
    mask_reject = (theta >= 2 * alpha) & (~only_aff_mask)

    delta = alpha + theta / 2.0
    hybrid_embed = (
        torch.cos(delta)[:, None] * neg_embed
        + (torch.sin(delta) / torch.sin(theta))[:, None]
        * (aff_embed - a_T_n[:, None] * neg_embed)
    )

    text_features[mask_pass] = hybrid_embed[mask_pass]
    text_features[mask_reject] = aff_embed[mask_reject]

    return text_features  # (B, D)

CLIP_VISION_ID = "openai/clip-vit-large-patch14"
clip_processor = AutoProcessor.from_pretrained(CLIP_VISION_ID)
clip_vision    = CLIPModel.from_pretrained(CLIP_VISION_ID).to(DEVICE).eval()

@torch.no_grad()
def evaluate_mcq(
    model,
    dataloader: DataLoader,
    device: torch.device,
    threshold: float = 0.92,
    tokenize_fn: Optional[Callable[[list], torch.Tensor]] = None,
    return_per_template: bool = True,
) -> Tuple[float, Optional[Dict[str, float]]]:
    model.eval()
    all_accs = []
    all_templates = []

    for (
        image,
        caption_0_aff,
        caption_0_neg,
        caption_1_aff,
        caption_1_neg,
        caption_2_aff,
        caption_2_neg,
        caption_3_aff,
        caption_3_neg,
        label,
        template,
    ) in tqdm(dataloader, total=len(dataloader), desc="Evaluating"):
        image = image.to(device, non_blocking=True)
        label = label.to(device, non_blocking=True)
        img_inputs = clip_processor(images=image, return_tensors="pt").to(DEVICE)
        image_embed = F.normalize(clip_vision.get_image_features(**img_inputs))
        cap0 = text_embedding_compute(
            caption_0_aff, caption_0_neg, model, device, threshold, tokenize_fn
        )
        cap1 = text_embedding_compute(
            caption_1_aff, caption_1_neg, model, device, threshold, tokenize_fn
        )
        cap2 = text_embedding_compute(
            caption_2_aff, caption_2_neg, model, device, threshold, tokenize_fn
        )
        cap3 = text_embedding_compute(
            caption_3_aff, caption_3_neg, model, device, threshold, tokenize_fn
        )

        captions_embed = torch.stack([cap0, cap1, cap2, cap3], dim=-2)

        scores = torch.squeeze(
            image_embed[:, None, :] @ captions_embed.permute(0, 2, 1), dim=-2
        )
        preds = scores.argmax(dim=-1)

        batch_acc = (preds == label).float().cpu().numpy()
        all_accs.append(batch_acc)
        all_templates.extend(template)

    accs = np.concatenate(all_accs, axis=0)
    mean_acc = float(accs.mean())

    if not return_per_template:
        return mean_acc, None

    templates = np.array(all_templates)
    accs_per_template = {}
    for acc, tmpl in zip(accs, templates):
        accs_per_template.setdefault(tmpl, []).append(acc)
    accs_per_template = {k: float(np.mean(v)) for k, v in accs_per_template.items()}

    return mean_acc, accs_per_template

def parse_caption_pair(val):
    if isinstance(val, (list, tuple)) and len(val) == 2:
        return list(val)

    if not isinstance(val, str):
        return ["A photo of something", "A photo of a "]

    s = val.strip()
    if not s:
        return ["A photo of something", "A photo of a "]

    try:
        parsed = ast.literal_eval(s)
        if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
            return list(parsed)
    except Exception:
        pass

    trans_table = str.maketrans({
        "（": "(",
        "）": ")",
        "，": ",",
        "“": '"',
        "”": '"',
    })
    s_norm = s.translate(trans_table)
    try:
        parsed = ast.literal_eval(s_norm)
        if isinstance(parsed, (list, tuple)) and len(parsed) == 2:
            return list(parsed)
    except Exception:
        pass

    parts = [p.strip(" ()[]\"' ") for p in s_norm.split(",")]
    parts = [p for p in parts if p]

    if len(parts) == 1:
        return [parts[0], "A photo of a "]
    elif len(parts) >= 2:
        return [parts[0], parts[1]]

    return ["A photo of something", "A photo of a "]

device = "cuda" if torch.cuda.is_available() else "cpu"

model, preprocess = clip.load("ViT-B/32", device=device, jit=False)



model.load_state_dict(torch.load("/content/drive/MyDrive/models/ConCLIP/conclip_b32_openclip_version.pt", weights_only=False))

from google.colab import drive
drive.mount('/content/drive')

dataloader = build_coco_mcq_dataloader(
    data_csv_path="/content/drive/MyDrive/spanish.csv",
    caps_csv_path="/content/drive/MyDrive/COCO_val_mcq_LLM_captions__es.csv",
    batch_size=1024,
    num_workers=0,
    base_preprocess=preprocess,
)

mean_acc, accs_per_template = evaluate_mcq(
    mclip,
    dataloader,
    device=torch.device(device),
    threshold=0.92,
    tokenize_fn=mclip_tok,
)

print("Mean accuracy:", mean_acc)
print("Per-template:", accs_per_template)

!pip -q install multilingual-clip

DEVICE = 'cuda'

from transformers import (
    CLIPModel,
    AutoProcessor,
    AutoTokenizer,
)
from multilingual_clip import pt_multilingual_clip

TEXT_MODEL_ID = "M-CLIP/XLM-Roberta-Large-Vit-L-14"
CLIP_VISION_ID = "openai/clip-vit-large-patch14"
mclip = pt_multilingual_clip.MultilingualCLIP.from_pretrained(TEXT_MODEL_ID).to(DEVICE).eval()
mclip_tok = AutoTokenizer.from_pretrained(TEXT_MODEL_ID)



import matplotlib.pyplot as plt
import numpy as np

languages = ["English", "Chinese", "Arabic", "Greek", "Russian", "Tagalog", "Spanish"]

data_clip = {
    "English": 0.6934393048286438,
    "Chinese": 0.3429151177406311,
    "Greek": 0.2850862443447113,
    "Russian": 0.27088266611099243,
    "Arabic": 0.22522826492786407,
    "Tagalog": 0.550896167755127,
    "Spanish": 0.5811633467674255
}

data_negclip = {
    "English": 0.6871829628944397,
    "Chinese": 0.3882313072681427,
    "Greek": 0.45427435636520386,
    "Russian": 0.2888062298297882,
    "Arabic": 0.18109570443630219,
    "Tagalog": 0.549881637096405,
    "Spanish": 0.5791342854499817
}

def create_chart(data_dict, title, ax):
    values = [data_dict[lang] for lang in languages]

    bar_color = '#9e2a2f'

    bars = ax.bar(languages, values, color=bar_color, width=0.75, edgecolor='#5c181b')

    ax.set_ylim(0, 0.8)
    ax.set_ylabel("Accuracy", fontsize=12, labelpad=10)
    ax.set_title(title, fontsize=16, fontweight='bold', pad=15, color='#0f1626')

    ax.grid(axis='y', linestyle='--', alpha=0.5, color='gray', zorder=0)
    ax.set_axisbelow(True)

    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_color('gray')
    ax.spines['bottom'].set_color('gray')

    ax.set_xticklabels(languages, rotation=25, ha='right', fontsize=11)

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

create_chart(data_clip, "CLIP_CC12M_NegFull", ax1)
create_chart(data_negclip, "NegCLIP (ViT-B-32)", ax2)

plt.tight_layout()
plt.show()